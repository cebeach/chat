Phase 2 — Feature Roadmap for AI Chat
======================================

Context
-------
Phase 1 delivered a working terminal chat app with streaming responses, slash
commands, and conversation history (in-memory only). Phase 2 adds persistence,
better UX, and quality-of-life features — all still zero-network, air-gap safe,
and with no new dependencies beyond what's already installed (requests, rich).

New Features
------------

1. Config File (config.py — new file, chat.py)
   - TOML config at ~/.config/chat/config.toml (stdlib tomllib, Python 3.11+)
   - Settings: default_model, system_prompt, ollama_url, conversations_dir
   - CLI args override config file values
   - New command: /config — display current effective settings

2. Save/Load Conversations (conversation.py, chat.py)
   - save(filepath) and load(filepath) methods on Conversation
   - JSON format: {"system_prompt": "...", "model": "...", "messages": [...]}
   - Storage: ~/.local/share/chat/conversations/
   - Auto-create directory on first save
   - New commands:
     - /save [name]        — save conversation (default name: timestamp)
     - /load <name>        — load a saved conversation
     - /conversations      — list saved conversations

3. Input History with Readline (chat.py, ui.py)
   - stdlib readline module (no new dependency)
   - Arrow-key history navigation (up/down)
   - Persist to ~/.local/share/chat/history
   - Max history size: 1000 lines

4. Markdown Rendering of Responses (ui.py)
   - rich.markdown.Markdown is already imported but unused
   - After streaming, re-render full response as Rich Markdown
   - Proper headings, code blocks, lists, bold/italic

5. Token/Context Tracking (ollama_client.py, ui.py)
   - Parse eval_count, eval_duration, prompt_eval_count from Ollama response
   - Display tokens generated and tokens/sec after each response
   - New command: /stats — toggle stats display on/off

6. Multiline Input (ui.py, chat.py)
   - Type """ to enter multiline mode, """ again to submit
   - Visual indicator when in multiline mode

Files Modified
--------------
  conversation.py   — save(), load(), list_saved() methods
  ollama_client.py  — parse and return token stats metadata from stream
  ui.py             — markdown rendering, readline, multiline input, stats
  chat.py           — new commands, config loading, readline setup
  config.py (new)   — config file parsing and defaults

No New Dependencies
-------------------
All features use Python stdlib (json, readline, tomllib, pathlib, os) plus
existing requests and rich.

Implementation Order
--------------------
1. Config file         — foundational, other features reference it for paths
2. Save/load convos    — high value, uses config for paths
3. Input history       — small change, big UX improvement
4. Markdown rendering  — already half-wired, quick win
5. Token stats         — parse existing API data, display optionally
6. Multiline input     — nice-to-have, least critical

Verification
------------
1. Create config file, verify settings load and CLI args override them
2. Chat, /save test1, /clear, /load test1 — verify history restored
3. Press up-arrow — verify previous input recalled
4. Ask model to generate markdown — verify it renders with formatting
5. Send a message, check stats line shows tokens/sec
6. Type """, enter multiple lines, type """ — verify sent as single message
